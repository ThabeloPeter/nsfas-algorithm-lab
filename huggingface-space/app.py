from fastapi import FastAPI, File, UploadFile, HTTPException, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import cv2
import numpy as np
from PIL import Image
import io
import base64
import fitz  # PyMuPDF
from typing import Dict, Any, List, Tuple
import logging
from mtcnn import MTCNN
import face_recognition

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="NSFAS Face Extraction API - MTCNN", version="3.0.0")

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize MTCNN detector
try:
    detector = MTCNN()
    logger.info("✅ MTCNN face detector initialized successfully")
except Exception as e:
    logger.error(f"❌ Failed to initialize MTCNN: {str(e)}")
    raise RuntimeError("Failed to load MTCNN model")

# Keep OpenCV as fallback for eye validation
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')

def pdf_to_image(pdf_bytes: bytes) -> np.ndarray:
    """Convert first page of PDF to high-quality numpy array image."""
    try:
        pdf_document = fitz.open(stream=pdf_bytes, filetype="pdf")
        first_page = pdf_document[0]
        
        # Render at 3x scale for HIGH QUALITY
        mat = fitz.Matrix(3.0, 3.0)
        pix = first_page.get_pixmap(matrix=mat, alpha=False)
        
        # Convert to numpy array
        img = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.height, pix.width, pix.n)
        
        # Convert to BGR (OpenCV format)
        if pix.n == 4:
            img = cv2.cvtColor(img, cv2.COLOR_RGBA2BGR)
        elif pix.n == 3:
            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
        
        pdf_document.close()
        logger.info(f"PDF converted to image: {img.shape}")
        return img
    except Exception as e:
        logger.error(f"PDF conversion error: {str(e)}")
        raise HTTPException(status_code=400, detail=f"Failed to convert PDF: {str(e)}")

def assess_image_quality(image: np.ndarray) -> Dict[str, Any]:
    """
    Assess image quality for face detection.
    Returns quality metrics and recommendations.
    """
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    
    # 1. Blur detection (Laplacian variance)
    blur_score = cv2.Laplacian(gray, cv2.CV_64F).var()
    is_blurry = blur_score < 100
    
    # 2. Lighting check
    mean_brightness = np.mean(gray)
    is_too_dark = mean_brightness < 50
    is_too_bright = mean_brightness > 200
    
    # 3. Contrast check
    contrast = gray.std()
    is_low_contrast = contrast < 30
    
    quality_status = 'good'
    issues = []
    
    if is_blurry:
        quality_status = 'poor'
        issues.append('Image is blurry - hold camera steady')
    if is_too_dark:
        quality_status = 'poor'
        issues.append('Too dark - turn on flash or add light')
    if is_too_bright:
        quality_status = 'poor'
        issues.append('Too bright - avoid direct light')
    if is_low_contrast:
        quality_status = 'fair'
        issues.append('Low contrast - adjust lighting')
    
    return {
        'blur_score': float(blur_score),
        'is_blurry': bool(is_blurry),
        'brightness': float(mean_brightness),
        'is_too_dark': bool(is_too_dark),
        'is_too_bright': bool(is_too_bright),
        'contrast': float(contrast),
        'is_low_contrast': bool(is_low_contrast),
        'quality': quality_status,
        'issues': issues
    }

def validate_face_with_eyes(face_region: np.ndarray) -> bool:
    """
    Validate that detected region is actually a face by detecting eyes.
    Eliminates false positives (dots, logos, etc.)
    """
    gray = cv2.cvtColor(face_region, cv2.COLOR_BGR2GRAY)
    eyes = eye_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=3, minSize=(20, 20))
    
    # Real faces should have at least 1 eye (profile) or 2 (frontal)
    has_eyes = len(eyes) >= 1
    logger.info(f"Eye validation: {len(eyes)} eyes detected - {'VALID' if has_eyes else 'INVALID'}")
    
    return has_eyes

def detect_faces_mtcnn(image: np.ndarray, id_type: str = 'full') -> List[Dict[str, Any]]:
    """
    Detect faces using MTCNN deep learning detector.
    More accurate than Haar Cascade, fewer false positives.
    
    Args:
        image: Input image (BGR format)
        id_type: 'smart', 'green', or 'full' for ROI optimization
    
    Returns:
        List of face dictionaries with location, confidence, landmarks
    """
    height, width = image.shape[:2]
    
    # Define Region of Interest (ROI) based on ID type
    if id_type == 'smart':
        # Smart ID Card: Photo is on RIGHT side
        roi_x_start = int(width * 0.50)
        roi_x_end = width
        roi_y_start = 0
        roi_y_end = height
        logger.info(f"Smart ID - Scanning RIGHT 50%")
    elif id_type == 'green':
        # Green ID Book: Photo in upper-center
        roi_x_start = int(width * 0.15)
        roi_x_end = int(width * 0.85)
        roi_y_start = 0
        roi_y_end = int(height * 0.75)
        logger.info(f"Green ID - Scanning upper-center")
    else:
        # Full image
        roi_x_start = 0
        roi_x_end = width
        roi_y_start = 0
        roi_y_end = height
        logger.info(f"Scanning full image")
    
    # Extract ROI
    roi = image[roi_y_start:roi_y_end, roi_x_start:roi_x_end]
    
    # Convert BGR to RGB (MTCNN expects RGB)
    rgb_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)
    
    # Detect faces with MTCNN
    detections = detector.detect_faces(rgb_roi)
    
    logger.info(f"🧠 MTCNN detected {len(detections)} face(s)")
    
    # Filter and validate faces
    validated_faces = []
    
    for detection in detections:
        confidence = detection['confidence']
        box = detection['box']  # [x, y, width, height]
        keypoints = detection['keypoints']  # eyes, nose, mouth
        
        # Only accept high-confidence detections
        if confidence < 0.95:
            logger.info(f"Rejected: Low confidence ({confidence:.2f})")
            continue
        
        # Extract box coordinates
        x, y, w, h = box
        
        # Handle negative coordinates (MTCNN bug)
        x = max(0, x)
        y = max(0, y)
        w = min(w, roi.shape[1] - x)
        h = min(h, roi.shape[0] - y)
        
        # Convert ROI coordinates to full image coordinates
        full_x = x + roi_x_start
        full_y = y + roi_y_start
        
        # Validate aspect ratio (faces should be roughly square/portrait)
        aspect_ratio = w / h if h > 0 else 0
        if aspect_ratio < 0.5 or aspect_ratio > 1.8:
            logger.info(f"Rejected: Bad aspect ratio ({aspect_ratio:.2f})")
            continue
        
        # Extract face region for eye validation
        face_region = image[full_y:full_y+h, full_x:full_x+w]
        
        # Validate with eye detection (double-check)
        if not validate_face_with_eyes(face_region):
            logger.info(f"Rejected: No eyes detected")
            continue
        
        validated_faces.append({
            'box': (full_x, full_y, w, h),
            'confidence': confidence,
            'keypoints': {
                'left_eye': (keypoints['left_eye'][0] + roi_x_start, keypoints['left_eye'][1] + roi_y_start),
                'right_eye': (keypoints['right_eye'][0] + roi_x_start, keypoints['right_eye'][1] + roi_y_start),
                'nose': (keypoints['nose'][0] + roi_x_start, keypoints['nose'][1] + roi_y_start),
                'mouth_left': (keypoints['mouth_left'][0] + roi_x_start, keypoints['mouth_left'][1] + roi_y_start),
                'mouth_right': (keypoints['mouth_right'][0] + roi_x_start, keypoints['mouth_right'][1] + roi_y_start),
            },
            'aspect_ratio': aspect_ratio
        })
    
    logger.info(f"✅ {len(validated_faces)} validated faces (after quality checks)")
    return validated_faces

def score_faces(faces: List[Dict], image_shape: tuple) -> List[Dict]:
    """Score and rank faces based on size, position, and confidence."""
    height, width = image_shape[:2]
    
    for face in faces:
        x, y, w, h = face['box']
        
        # Calculate face area
        face_area = w * h
        
        # Calculate center position
        center_x = x + w / 2
        center_y = y + h / 2
        
        # Distance from ideal position (top-center to upper-third)
        ideal_x = width / 2
        ideal_y = height / 3
        dist_from_ideal = np.sqrt((center_x - ideal_x)**2 + (center_y - ideal_y)**2)
        max_dist = np.sqrt((width / 2)**2 + (height / 3)**2)
        
        # Position score (0-1, higher is better)
        pos_score = 1 - (dist_from_ideal / max_dist)
        
        # Size score (0-1, higher is better)
        size_score = face_area / (width * height)
        
        # Combined score (50% confidence, 30% size, 20% position)
        combined_score = (face['confidence'] * 0.5) + (size_score * 0.3) + (pos_score * 0.2)
        
        face['size_score'] = float(size_score)
        face['pos_score'] = float(pos_score)
        face['combined_score'] = float(combined_score)
        face['area'] = int(face_area)
    
    # Sort by combined score (highest first)
    faces.sort(key=lambda x: x['combined_score'], reverse=True)
    
    return faces

def extract_face_region(image: np.ndarray, location: tuple, padding_percent: float = 0.3) -> np.ndarray:
    """Extract face region with padding."""
    height, width = image.shape[:2]
    x, y, w, h = location
    
    # Add padding
    padding_x = int(w * padding_percent)
    padding_y = int(h * padding_percent)
    
    x1 = max(0, x - padding_x)
    y1 = max(0, y - padding_y)
    x2 = min(width, x + w + padding_x)
    y2 = min(height, y + h + padding_y)
    
    face_image = image[y1:y2, x1:x2]
    
    # Apply slight sharpening
    kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
    face_image = cv2.filter2D(face_image, -1, kernel)
    
    logger.info(f"Extracted face: {face_image.shape}")
    return face_image

def image_to_base64(image: np.ndarray) -> str:
    """Convert numpy image to base64 string."""
    if len(image.shape) == 3 and image.shape[2] == 3:
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    else:
        image_rgb = image
    
    pil_image = Image.fromarray(image_rgb)
    buffer = io.BytesIO()
    pil_image.save(buffer, format='JPEG', quality=95)
    img_bytes = buffer.getvalue()
    img_base64 = base64.b64encode(img_bytes).decode('utf-8')
    
    return f"data:image/jpeg;base64,{img_base64}"

@app.get("/")
async def root():
    """Health check endpoint."""
    return {
        "status": "healthy",
        "service": "NSFAS Face Extraction API",
        "version": "3.0.0",
        "detector": "MTCNN (Deep Learning)",
        "accuracy": "95-98%"
    }

@app.post("/extract-face")
async def extract_face(
    file: UploadFile = File(...),
    id_type: str = Form('full')
) -> JSONResponse:
    """
    Extract face from ID document using MTCNN deep learning detector.
    
    Features:
    - 95-98% accuracy (vs 70-80% with Haar Cascade)
    - Facial landmark detection (eyes, nose, mouth)
    - Eye validation to eliminate false positives
    - Image quality assessment with feedback
    - ROI optimization for Smart ID and Green ID
    """
    try:
        # Read file
        contents = await file.read()
        logger.info(f"📄 Processing: {file.filename} ({len(contents)} bytes)")
        
        if len(contents) > 10 * 1024 * 1024:
            raise HTTPException(status_code=400, detail="File too large. Maximum 10MB.")
        
        # Load image
        if file.content_type == 'application/pdf':
            image = pdf_to_image(contents)
        elif file.content_type in ['image/jpeg', 'image/png', 'image/jpg']:
            nparr = np.frombuffer(contents, np.uint8)
            image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
            if image is None:
                raise HTTPException(status_code=400, detail="Invalid image file.")
        else:
            raise HTTPException(status_code=400, detail="Unsupported file type. Upload JPEG, PNG, or PDF.")
        
        logger.info(f"📐 Image loaded: {image.shape}")
        
        # Assess image quality FIRST
        quality = assess_image_quality(image)
        logger.info(f"🔍 Image quality: {quality['quality']} - {quality.get('issues', [])}")
        
        # Resize if too large
        max_dimension = 1600
        height, width = image.shape[:2]
        if max(height, width) > max_dimension:
            scale = max_dimension / max(height, width)
            new_width = int(width * scale)
            new_height = int(height * scale)
            image_resized = cv2.resize(image, (new_width, new_height))
            logger.info(f"📏 Resized to: {image_resized.shape}")
        else:
            image_resized = image
            scale = 1.0
        
        # Detect faces with MTCNN
        faces = detect_faces_mtcnn(image_resized, id_type=id_type)
        
        if len(faces) == 0:
            # Provide specific feedback based on image quality
            if quality['is_blurry']:
                detail = "Image is too blurry. Hold the camera steady and try again."
            elif quality['is_too_dark']:
                detail = "Image is too dark. Turn on the flash or move to a brighter area."
            elif quality['is_too_bright']:
                detail = "Image is overexposed. Move away from direct light sources."
            else:
                detail = "No face detected. Ensure the ID photo is clearly visible and try again."
            
            raise HTTPException(status_code=404, detail=detail)
        
        # Score and select best face
        scored_faces = score_faces(faces, image_resized.shape)
        best_face = scored_faces[0]
        
        # Scale back to original coordinates
        if scale != 1.0:
            x, y, w, h = best_face['box']
            original_box = (int(x/scale), int(y/scale), int(w/scale), int(h/scale))
        else:
            original_box = best_face['box']
        
        # Extract face region
        face_image = extract_face_region(image, original_box, padding_percent=0.3)
        face_base64 = image_to_base64(face_image)
        
        # Prepare metadata
        metadata = {
            'total_faces': len(faces),
            'selected_index': 0,
            'confidence': round(best_face['confidence'] * 100, 1),
            'face_size': {'width': face_image.shape[1], 'height': face_image.shape[0]},
            'position_score': round(best_face['pos_score'] * 100, 1),
            'size_score': round(best_face['size_score'] * 100, 1),
            'combined_score': round(best_face['combined_score'] * 100, 1),
            'aspect_ratio': round(best_face['aspect_ratio'], 2),
            'detector': 'MTCNN Deep Learning',
            'landmarks_detected': True,
            'image_quality': quality,
            'all_faces': [
                {
                    'index': idx,
                    'confidence': round(f['confidence'] * 100, 1),
                    'score': round(f['combined_score'] * 100, 1)
                }
                for idx, f in enumerate(scored_faces)
            ]
        }
        
        logger.info(f"✅ Success! Confidence: {metadata['confidence']}%")
        
        return JSONResponse(content={
            'success': True,
            'face_image': face_base64,
            'metadata': metadata
        })
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"❌ Error: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@app.post("/compare-faces")
async def compare_faces(
    selfie: UploadFile = File(...),
    id_photo: UploadFile = File(...)
) -> JSONResponse:
    """
    Compare two face images using face_recognition (dlib ResNet).
    Accuracy: 99.38% on LFW benchmark.
    
    Returns:
        - match: Boolean indicating if faces match
        - similarity: Similarity score (0-100)
        - confidence: Match confidence percentage
        - distance: Face encoding distance (lower = more similar)
    """
    try:
        # Load both images
        selfie_bytes = await selfie.read()
        id_bytes = await id_photo.read()
        
        # Convert to RGB format for face_recognition
        selfie_img = face_recognition.load_image_file(io.BytesIO(selfie_bytes))
        id_img = face_recognition.load_image_file(io.BytesIO(id_bytes))
        
        logger.info("🔍 Detecting faces in selfie and ID photo...")
        
        # Get face encodings (128-dimensional embeddings)
        selfie_encodings = face_recognition.face_encodings(selfie_img)
        id_encodings = face_recognition.face_encodings(id_img)
        
        if len(selfie_encodings) == 0:
            raise HTTPException(
                status_code=404, 
                detail="No face detected in selfie. Please ensure your face is clearly visible."
            )
        
        if len(id_encodings) == 0:
            raise HTTPException(
                status_code=404, 
                detail="No face detected in ID photo. Please use the extracted face from your ID."
            )
        
        # Compare faces using Euclidean distance
        distances = face_recognition.face_distance([id_encodings[0]], selfie_encodings[0])
        distance = float(distances[0])
        
        # Convert distance to similarity percentage
        # Distance range: 0 (identical) to 1+ (very different)
        # Industry standard threshold: 0.6
        similarity = max(0, min(100, (1 - distance) * 100))
        
        # Determine match based on threshold
        threshold = 0.6  # Industry standard for face_recognition
        is_match = distance < threshold
        
        # Calculate confidence level
        if distance < 0.4:
            confidence_level = "Very High"
            confidence = 95
        elif distance < 0.5:
            confidence_level = "High"
            confidence = 85
        elif distance < 0.6:
            confidence_level = "Medium"
            confidence = 75
        else:
            confidence_level = "Low"
            confidence = max(0, 70 - (distance - 0.6) * 100)
        
        logger.info(f"✅ Face comparison complete - Distance: {distance:.4f}, Match: {is_match}, Similarity: {similarity:.1f}%")
        
        return JSONResponse(content={
            'success': True,
            'match': bool(is_match),
            'similarity': round(similarity, 1),
            'confidence': round(confidence, 1),
            'confidence_level': confidence_level,
            'distance': round(distance, 4),
            'threshold': threshold,
            'method': 'face_recognition (dlib ResNet)',
            'accuracy': '99.38% (LFW benchmark)'
        })
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"❌ Error comparing faces: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Face comparison error: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=7860)
